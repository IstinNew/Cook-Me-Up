# -*- coding: utf-8 -*-
"""data_cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IxiXuOYMKIybSxeLV3Fv0KCFSm3kNoxm

#### **Cook-Me-Up Project: Simplifying Culinary Delights with Data**
**Data Cleaning**

This notebook documents the data cleaning process for the Cookmeup project. The dataset used contains information about various Indian foods, including their ingredients, diet type, preparation time, cooking time, flavor profile, course, state, and region. The goal is to clean the dataset by handling missing values, removing duplicates, and saving the cleaned data for further analysis.

The dataset is downloaded using the Kaggle API via the `kagglehub` library. After downloading, the dataset is loaded into a pandas DataFrame for cleaning. The cleaning process includes handling missing values, removing duplicates, and saving the cleaned data for further analysis.

#### **Global Configuration**
"""

import pandas as pd                                             # Importing the required libraries
import numpy as np                                              # Importing the required libraries
import matplotlib.pyplot as plt                                 # Importing the required libraries
import seaborn as sns                                           # Importing the required libraries
import scipy.stats as stats                                     # Importing the required libraries
from pydrive.auth import GoogleAuth                             # Importing the google drive authentication module(when needed)
from pydrive.drive import GoogleDrive                           # Importing the google drive module (when needed)

import warnings
warnings.filterwarnings('ignore')

"""#### **Load Dataset using Kaggle API**

The dataset can be downloaded using the Kaggle API. The following code demonstrates how to download and load the dataset using the Kaggle API:

"""

# Dataset Latest Version
path = kh.dataset_download("nehaprabhavalkar/indian-food-101")

# File path to the CSV file
file_path = f"{path}/indian_food.csv"

# Load the dataset
data = pd.read_csv(file_path)

print("Dataset loaded successfully!")

"""#### **Dataset Cleaning**
To handle missing values, remove duplicates, and save the cleaned data, the following steps are performed:
"""

# Handle Missing Values
data.fillna('Unknown', inplace=True)

# Remove Duplicates
data.drop_duplicates(inplace=True)

# Save Cleaned Data
cleaned_file_path = r'D:/WBS Coding/Bootcamp/Project Works/Final Project/Data/cleaned_data.csv'

data.to_csv(cleaned_file_path, index=False)

print("Cleaned data saved successfully!")

# Read the cleaned data file on GDrive
url_cleaned = "https://drive.google.com/file/d/12BM2d2Yv53dvf2b83gGlmzbtCKBt2Q6n/view?usp=sharing"
path = "https://drive.google.com/uc?export=download&id="+url_cleaned.split("/")[-2]
data = pd.read_csv(path)

"""#### **Initial Dataset Checkpoints**"""

# Display the first few rows of the dataset
print("Data Head:")
print(data.head())

# Display the column names
print("\nData Columns:")
print(data.columns)

# Display the summary of the DataFrame
print("\nData Info:")
print(data.info())

# Check if there are any missing values
print("\nMissing Values Check (isnull().any()):")
print(data.isnull().any())

# Display the number of missing values in each column
print("\nMissing Values Count (isnull().sum()):")
print(data.isnull().sum())

"""#### Additional Checkpoints"""

# Additional Checkpoint 1: Descriptive Statistics
print("\nDescriptive Statistics:")
print(data.describe())

# Additional Checkpoint 2: Unique Values in Categorical Columns
print("\nUnique Values in Categorical Columns:")
for col in data.select_dtypes(include=['object']).columns:
    print(f"\nColumn: {col}")
    print(data[col].unique())